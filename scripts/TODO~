#_______________________________________#

ajouter la source dans la sortie en nohup

#_________ en cours ____________________#

différencier les retours de textes vides et les erreurs de timeout.
Pour les erreurs de time out il n'y a pas de problème, le texte est remplacé par le champ error_timeout

par contre quand le texte retourné est vide pour le moment rien n'est fait et malheureusement pour le moment ça génère des erreurs de formatage de fichiers xml.

La cause n'est pas directement connu, cela peut venir certainement du xpath qui n'est pas adapté à certain contenus.

Il faudra tester ces fichiers qui sont assez nombreux et proposer au minimum une stratégie pour ne pas les écrire dans le xml de sortie ou au minimum pour correctement fermer les balises xml.





#______________ DONE _____________#

Probleme de timout en utilisant le urllib2,peut-etre que request à tous les étages est plus indiqué.

gerere le raise excepttion par un try




#______________ DONE à vérifier par l'exemple ___________________#

faire passer l'encodage du fichier xml en utf-8 et l'encodage de l'écriture également.
comprendre pourquoi je n'ai pas le résultats dans le format escmpté.
J'écris dans le fichier avec du iso et je suis obligé de changé l'affichage en ut8 pour le visualiser correctement? WTF

proposition de changer le format d'écriture et de changer l'entête en UTF8

pour l'instant je fais un write().encore(iso) avec une entete de xml en utf8 et j'ouvre en utf8

le problème vient peut etre du fait que je n'encode pas l'entrée?
http://sametmax.com/lencoding-en-python-une-bonne-fois-pour-toute/


a voir avec : 
resource = urllib.request.urlopen(an_url)
content =  resource.read().decode(resource.headers.get_content_charset())


ou 

import urllib2
import chardet

def fetch(url):
 try:
    result = urllib2.urlopen(url)
    rawdata = result.read()
    encoding = chardet.detect(rawdata)
    return rawdata.decode(encoding['encoding'])

 except urllib2.URLError, e:
    handleError(e)


ou 

def read_url(url):
    reader_req = urllib2.Request(url)
    reader_resp = urllib2.urlopen(reader_req)
    reader_resp_content = reader_resp.read()
    reader_resp.close()

    try:
        return reader_resp_content.decode('utf-8')
    except:
        pass

    try:
        iso_string = reader_resp_content.decode('iso-8859-1')
        print 'UTF-8 decoding failed, but ISO-8859-1 decoding succeeded'
        return iso_string 
    except Exception, e:
        print e
        raise

sinon il y a request qui fait tout tout seul
r = requests.get('https://api.github.com/user', auth=('user', 'pass'))
t = r.text

sur 

http://docs.python-requests.org/en/latest/
https://gist.github.com/kennethreitz/973705

il y a aussi scrapy et beautiful soup mais je devrais m'en sortir





#________ verbose ajouté, doc a faire avec sphinx eventuellement___________________#

ajouter le verbose avec action=store_true dans argparse et faire une doc un peu plus précise.






#___ génération liste modifiée, test sur format et redondance à ajouter, readme dans l'entete_______#

Modifier le script de génération des liste de dates de manière à générer des requêtes sur une étendue plustôt qu'une date unique.
indiquer le syntaxe dans un README ou dans le help

éviter ainsi d'avoir deux fois le même article retourné pour une même entité, et augmenter dans le même temps, la taille des fichiers xml.




#____________DONE a priori, pas fait pour les cotes _______________#

virer les balises <> dans le corps du texte, les remplacer par des &lg; ect... idem pour les cotes simples et doubles etc...




#___________________________#

indiquer dans le log à partir de quelle page s'arrete la recherche quand il y a plusieurs page de resultats de recherche proposés




#____________________________#

dans le xml, si je peux avoir la vraie requete plutôt que la première page c'est bien




#_______DONE _____________________#

comprendre pourquoi sur le toshiba je suis obligé d'ouvrer les fichiers en utf8 pour les avoir corrects, et pourquoi lors de l'écriture au boulot je devais indiquer iso8859-1

#_______DONE____________#

changer les & en &amp; dans les chemins d'acces des xml



#____________________________#

Ajouter la date et la source de la requete (liberation) dans un attribut de la balise <doc>, histoire de pouvoir tracer l'origine




#________DONE___________#

ajouter dans le ficher liste done une syntaxe
*~~*
debut="1"
fin="2"
lastDone="-1"
*~~*

afin de pouvoir paraléliser à grande échelle le traitement de l'extraction et exploiter au mieux les identifiants.
Peut être à terme, permettre de multi-threader le traitement.

gérer le cas ou le fichier est vide, soit par un message d'erreur, soit par une valeur par défaut.
Dans ce cas, il faut charger d'abord le fichier de requête est ensuite mettre à jour les bornes de début et de fin en fonction de la taille de la liste de requetes.



#____________________________#



editer un log avec les url de recherche et le nombre d'url d'article extrait afin de faire un échantillonage des sortie ne donant rien.

idQuery : url de recherche : nb de sortie >> fichierLog portant le nom du processus avec os.getpgid(pid)




#____________________________#


je voudrais bien avoir plus d'info sur les error raised par les requêtes au serveur




#____________________________#

J'ai observé que certaines url d'article conduisent à un contenu texte vide ? pour l'instant je ne l'ai observé qu'une seule fois, pourquoi?




#___________DONE avec un try except et un pass _________________#

virer l'affichage de la sortie d'erreur pour le mkdir

import os, errno
def mkdir_p(path):
    try:
        os.makedirs(path)
    except OSError as exc: # Python >2.5
        if exc.errno == errno.EEXIST and os.path.isdir(path):
            pass
        else: raise



def make_sure_path_exists(path):
    try:
        os.makedirs(path)
    except OSError as exception:
        if exception.errno != errno.EEXIST:
            raise

sinon, faire un if not exists du path devrait suffir.

#______________________________#
